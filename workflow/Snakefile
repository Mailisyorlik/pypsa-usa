# Description: Snakefile for the new workflow

# -------------------------- Imports and Parameters -------------------------- #

from os.path import normpath

FIGURES_SINGLE = [
    "capacity_map",
    "capacity_bar",
    "operation_map",
    "operation_bar",
    "operation_area",
    "cost_bar",
]

# --------------------------- Workflow constraints --------------------------- #

localrules:
    # dag,
    # report,
    clean,

wildcard_constraints:
    interconnect="usa|texas|western|eastern",
    simpl="[a-zA-Z0-9]*|all",
    clusters="[0-9]+m?|all",
    ll="(v|c)([0-9\.]+|opt|all)|all",
    opts="[-+a-zA-Z0-9\.]*",


# -------------------------- Config and Subworkflows ------------------------- #

# Merge subworkflow configs and main config
configfile: "config/config.yaml"
configfile: "config/config.cluster.yaml"


# ----------------------------------- Rules ---------------------------------- #


rule all:
    input:
        expand(
            "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_{figure}.pdf",
            **config["scenarios"]["all"],
            figure=FIGURES_SINGLE
        ),

rule test:
    input:
        expand(
            "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_{figure}.pdf",
            **config["scenarios"]["test"],
            figure=FIGURES_SINGLE
        ),


################# ----------- Rules to Retrieve Data ---------- #################

DATAFILES = [
    "bus.csv",
    "sub.csv",
    "bus2sub.csv",
    "branch.csv",
    "dcline.csv",
    "demand.csv",
    "plant.csv",
    "solar.csv",
    "wind.csv",
    "hydro.csv",
    "zone.csv",
]

rule retrieve_data_from_zenodo:
    output:
        expand("data/base_grid/{file}", file=DATAFILES),
    log:
        "logs/retrieve_data_from_zenodo.log",
    script:
        "scripts/retrieve_data_from_zenodo.py"

rule retrieve_forecast_data:
    output:
        # expand("data/base_grid/{file}", file=DATAFILES), # need to rename the files that are output from this script
    log:
        "logs/retrieve_forecast_data.log",
    script:
        "scripts/retrieve_forecast_data.py"        

rule retrieve_historical_load_data:
    output:
        # expand("data/base_grid/{file}", file=DATAFILES), # need to rename the files that are output from this script
    log:
        "logs/retrieve_historical_load_data.log",
    script:
        "scripts/retrieve_historical_load_data.py"


################# ----------- Rules to Build Network ---------- #################

rule build_shapes:
    params:
        source_states_shapes="admin_1_states_provinces",
        source_offshore_shapes=config["offshore_shape"],
        buffer_distance=200000,
        balancing_authorities=config["balancing_authorities"],
    input:
        zone="data/base_grid/zone.csv",
    output:
        country_shapes="resources/{interconnect}/country_shapes.geojson",
        ba_region_shapes="resources/{interconnect}/ba_region_shapes.geojson",
        offshore_shapes="resources/{interconnect}/offshore_shapes.geojson",
    log:
        "logs/build_shapes_{interconnect}.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_shapes.py"

rule build_base_network:
    input:
        buses="data/base_grid/bus.csv",
        lines="data/base_grid/branch.csv",
        links="data/base_grid/dcline.csv",
        bus2sub="data/base_grid/bus2sub.csv",
        sub="data/base_grid/sub.csv",
        tech_costs="repo_data/pypsa_eur_costs.csv",
        ba_region_shapes="resources/{interconnect}/ba_region_shapes.geojson",
        offshore_shapes="resources/{interconnect}/offshore_shapes.geojson",
    output:
        bus2sub="data/base_grid/{interconnect}/bus2sub.csv",
        sub="data/base_grid/{interconnect}/sub.csv",
        network="resources/{interconnect}/elec_base_network.nc",
    log:
        "logs/create_network/{interconnect}.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/build_base_network.py"

# ads_files = [
#     "Load_2030.csv",
#     "Solar_2030.csv",
#     "Wind_2030.csv",
#     "Load_2032.csv",
#     "Solar_2032.csv",
#     "Wind_2032.csv",
# ]

rule build_load_data:
    input:
        network="resources/{interconnect}/elec_base_network.nc",
        demand_breakthrough_2016="data/base_grid/demand.csv",
        ads_2032 = 'resources/WECC_ADS/downloads/2032/Public Data/Hourly Profiles in CSV format',
        ads_2030 = 'resources/WECC_ADS/downloads/2030/WECC 2030 ADS PCM 2020-12-16 (V1.5) Public Data/CSV Shape Files',
    output:
        # demand_ads= expand("resources/demand_ads/{file}", file=ads_files),
        network="resources/{interconnect}/elec_base_network_load.nc",
    log:
        "logs/build_load_data/{interconnect}.log",
    script:
        "scripts/build_load_data.py"

# rule build_powerplants:
#  input:
#         base_network="resources/{interconnect}/elec.nc",
#         plants="data/base_grid/plant.csv",
#         wind="data/base_grid/wind.csv",
#         solar="data/base_grid/solar.csv",
#         hydro="data/base_grid/hydro.csv",
#         tech_costs="repo_data/pypsa_eur_costs.csv",
#     output:
#         network="resources/{interconnect}/elec.nc",
#     log:
#         "logs/create_network/{interconnect}.log",
#     threads: 4
#     resources:
#         mem=500,
#     script:
#         "scripts/build_powerplants.py"


################# ----------- Rules to Simplify Network ---------- #################
rule simplify_network:
    input:
        bus2sub="data/base_grid/{interconnect}/bus2sub.csv",
        sub="data/base_grid/{interconnect}/sub.csv",
        network="resources/{interconnect}/elec_base_network_load.nc",
    output:
        network="resources/{interconnect}/elec_s.nc",
    log:
        "logs/simplify_network/{interconnect}/elec_s.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/simplify_network.py"

rule build_bus_regions:
    params:
        balancing_authorities=config["balancing_authorities"],
    input:
        country_shapes="resources/{interconnect}/country_shapes.geojson",
        ba_region_shapes="resources/{interconnect}/ba_region_shapes.geojson",
        offshore_shapes="resources/{interconnect}/offshore_shapes.geojson",
        base_network="resources/{interconnect}/elec_s.nc",
    output:
        regions_onshore="resources/{interconnect}/regions_onshore_s.geojson",
        regions_offshore="resources/{interconnect}/regions_offshore_s.geojson",
        network="resources/{interconnect}/elec_ss.nc",
    log:
        "logs/{interconnect}/build_bus_regions_s.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_bus_regions.py"


rule cluster_network:
    input:
        network="resources/{interconnect}/elec_ss.nc",
        regions_onshore="resources/{interconnect}/regions_onshore_s.geojson",
        regions_offshore="resources/{interconnect}/regions_offshore_s.geojson",
        busmap="data/base_grid/{interconnect}/bus2sub.csv",
        custom_busmap=(
            "data/{interconnect}/custom_busmap_{clusters}.csv"
            if config["enable"].get("custom_busmap", False)
            else []
        ),
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        network="resources/{interconnect}/elec_s_{clusters}.nc",
        regions_onshore="resources/{interconnect}/regions_onshore_s_{clusters}.geojson",
        regions_offshore="resources/{interconnect}/regions_offshore_s_{clusters}.geojson",
        busmap="resources/{interconnect}/busmap_s_{clusters}.csv",
        linemap="resources/{interconnect}/linemap_s_{clusters}.csv",
    log:
        "logs/cluster_network/{interconnect}/elec_s_{clusters}.log",
    benchmark:
        "benchmarks/cluster_network/{interconnect}/elec_s_{clusters}"
    threads: 1
    resources:
        mem_mb=6000,
    script:
        "scripts/cluster_network_eur.py"


################# ----------- Rules to Optimize ---------- #################


rule add_extra_components:
    input:
        network="resources/{interconnect}/elec_s_{clusters}.nc",
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        "resources/{interconnect}/elec_s_{clusters}_ec.nc",
    log:
        "logs/add_extra_components/{interconnect}/elec_s_{clusters}_ec.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/add_extra_components.py"

rule prepare_network:
    input:
        network="resources/{interconnect}/elec_s_{clusters}_ec.nc",
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        "resources/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver="logs/prepare_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.log",
    threads: 4
    resources:
        mem=5000,
    log:
        "logs/prepare_network",
    script:
        "scripts/prepare_network.py"

def memory(w):
    factor = 3.0
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)h$", o, re.IGNORECASE)
        if m is not None:
            factor /= int(m.group(1))
            break
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)seg$", o, re.IGNORECASE)
        if m is not None:
            factor *= int(m.group(1)) / 8760
            break
    if w.clusters.endswith("m"):
        return int(factor * (18000 + 180 * int(w.clusters[:-1])))
    elif w.clusters == "all":
        return int(factor * (18000 + 180 * 4000))
    else:
        return int(factor * (10000 + 195 * int(w.clusters)))


rule solve_network:
    input:
        "resources/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    output:
        "results/{interconnect}/networks/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver=normpath(
            "logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_solver.log"
        ),
        python="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_python.log",
        memory="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_memory.log",
    benchmark:
        "benchmarks/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}"
    threads: 8
    resources:
        mem_mb=memory,
    script:
        "scripts/solve_network.py"


rule plot_figures_single:
    input:
        network="results/{interconnect}/networks/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
        regions_onshore="resources/{interconnect}/regions_onshore_s_{clusters}.geojson",
        regions_offshore="resources/{interconnect}/regions_offshore_s_{clusters}.geojson",
    output:
        **{
            fig: "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_%s.pdf"
            % fig
            for fig in FIGURES_SINGLE
        },
    notebook:
        "notebooks/plot-results.py.ipynb"

rule sync:
    params:
        cluster=config["cluster"],
    shell:
        """
        rsync -uvarh --no-g --ignore-missing-args --files-from=.sync-send . {params.cluster}
        rsync -uvarh --no-g --ignore-missing-args --files-from=.sync-receive {params.cluster} .
        """

rule report:
    message:
        "Compile report."
    input:
        tex="report/report.tex",
        bib="report/references.bib",
    output:
        "report/report.pdf",
    shell:
        """
        pdflatex {input.tex}
        bibtex {input.bib})
        pdflatex {input.tex}
        pdflatex {input.tex}
        """

# Create DAG with- snakemake --dag results/western/networks/elec_s_40_ec_lvopt_Co2L1.0.nc -F | sed -n "/digraph/,/}/p" | dot -Tpng -o results/workflow.png
rule dag:
    message:
        "Plotting dependency graph of the workflow."
    output:
        dot="resources/dag.dot",
        pdf="resources/dag.pdf",
    shell:
        """
        snakemake --rulegraph > {output.dot}
        dot -Tpdf -o {output.pdf} {output.dot}
        """

rule clean:
    message:
        "Remove all build results but keep downloaded data."
    run:
        import shutil

        shutil.rmtree("resources", ignore_errors=True)
        shutil.rmtree("results", ignore_errors=True)
        print("Data downloaded to data/ has not been cleaned.")
