from snakemake.utils import min_version
min_version("6.0")

from shutil import copyfile, move, rmtree
from snakemake.remote.HTTP import RemoteProvider as HTTPRemoteProvider

HTTP = HTTPRemoteProvider()

# -------------------------- Imports and Parameters -------------------------- #

from os.path import normpath

FIGURES_SINGLE = [
    "capacity_map",
    "capacity_bar",
    "operation_map",
    "operation_bar",
    "operation_area",
    "cost_bar",
]

# --------------------------- Workflow constraints --------------------------- #

localrules:
    dag,
    # report,
    clean,

wildcard_constraints:
    interconnect="usa|texas|western|eastern",
    simpl="[a-zA-Z0-9]*|all",
    clusters="[0-9]+m?|all",
    ll="(v|c)([0-9\.]+|opt|all)|all",
    opts="[-+a-zA-Z0-9\.]*",


# -------------------------- Config and Subworkflows ------------------------- #

# Merge subworkflow configs and main config
configfile: "config/config.yaml"
configfile: "config/config.cluster.yaml"

ATLITE_NPROCESSES = config["atlite"].get("nprocesses", 4)

run = config.get("run", {})
RDIR = run["name"] + "/" if run.get("name") else ""
CDIR = RDIR if not run.get("shared_cutouts") else ""

LOGS = "logs/" + RDIR
BENCHMARKS = "benchmarks/" + RDIR
DATA = "data/" + RDIR
RESOURCES = "resources/" + RDIR if not run.get("shared_resources") else "resources/"
RESULTS = "results/" + RDIR

# ----------------------------------- Rules ---------------------------------- #


rule all:
    input:
        expand(
            "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_{figure}.pdf",
            **config["scenarios"]["all"],
            figure=FIGURES_SINGLE
        ),
        "repo_data/dag.png",

# rule test:
#     input:
#         expand(
#             "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_{figure}.pdf",
#             **config["scenarios"]["test"],
#             figure=FIGURES_SINGLE
#         ),


################# ----------- Rules to Retrieve Data ---------- #################

DATAFILES = [
    "bus.csv",
    "sub.csv",
    "bus2sub.csv",
    "branch.csv",
    "dcline.csv",
    "demand.csv",
    "plant.csv",
    "solar.csv",
    "wind.csv",
    "hydro.csv",
    "zone.csv",
]

rule retrieve_breakthrough_network_data:
    output:
        expand( DATA + "breakthrough_network/base_grid/{file}", file=DATAFILES),
    log:
        "logs/retrieve_breakthrough_network_data.log",
    script:
        "scripts/retrieve_data_from_zenodo.py"

rule retrieve_WECC_forecast_data:
    output:
        ads_2032 = directory(RESOURCES + "WECC_ADS/downloads/2032/Public Data/Hourly Profiles in CSV format"),
        ads_2030 = directory(RESOURCES + "WECC_ADS/downloads/2030/WECC 2030 ADS PCM 2020-12-16 (V1.5) Public Data/CSV Shape Files"),
    log:
        "logs/retrieve_WECC_forecast_data.log",
    script:
        "scripts/retrieve_forecast_data.py"        

DATAFILES_DMD = [
    "EIA_DMD_2015.csv",
    "EIA_DMD_2016.csv",
    "EIA_DMD_2017.csv",
    "EIA_DMD_2018.csv",
    "EIA_DMD_2019.csv",
    "EIA_DMD_2020.csv",
    "EIA_DMD_2021.csv",
    "EIA_DMD_2022.csv",
    "EIA_DMD_2023.csv",
    ]

rule retrieve_eia_data:
    output:
        expand(RESOURCES + "eia/{file}", file=DATAFILES_DMD),
    log:
        "logs/retrieve_historical_load_data.log",
    script:
        "scripts/retrieve_historical_load_data.py"


rule retrieve_ship_raster:
    input:
        HTTP.remote(
            "https://zenodo.org/record/6953563/files/shipdensity_global.zip",
            keep_local=True,
            static=True,
        ),
    output:
        DATA +"shipdensity_global.zip",
    log:
        LOGS + "retrieve_ship_raster.log",
    resources:
        mem_mb=5000,
    retries: 2
    run:
        move(input[0], output[0])

if config["enable"].get("retrieve_databundle", True):
    datafiles = [
    "eez/eez_v11.gpkg",
    "gebco/GEBCO_2021_TID.nc",
    "copernicus/PROBAV_LC100_global_v3.0.1_2019-nrt_Discrete-Classification-map_EPSG-4326.tif",
    ]

    rule retrieve_pypsa_earth_databundle:
        output:
            expand(DATA + "pypsa_earth/{file}", file=datafiles),
        log:
            LOGS + "retrieve_databundle.log",
        resources:
            mem_mb=1000,
        retries: 2
        script:
            "scripts/retrieve_pypsa_earth_databundle.py"

rule retrieve_natura_data:
    output:
        "data/natura.tiff",
    log:
        "logs/retrieve_natura_data.log",
    script:
        "scripts/retrieve_natura_data.py"


if config["enable"].get("retrieve_cutout", True):

    rule retrieve_cutout:
        input:
            HTTP.remote(
                "zenodo.org/record/8136996/files/cutout_western_{cutout}.nc"
                ,static=True),
        output:
            "cutouts/" + CDIR + "cutout_{interconnect}_{cutout}.nc",
        log:
            "logs/" + CDIR + "retrieve_cutout_{interconnect}_{cutout}.log",
        resources:
            mem_mb=5000,
        retries: 2
        run:
            move(input[0], output[0])
####"https://zenodo.org/record/8136996/files/cutout_western_ERA5_2019.nc",


################# ----------- Rules to Build Network ---------- #################

rule build_shapes:
    params:
        source_states_shapes="admin_1_states_provinces",
        source_offshore_shapes=config["offshore_shape"],
        buffer_distance=200000,
        balancing_areas=config["balancing_areas"],
    input:
        zone= DATA + "breakthrough_network/base_grid/zone.csv",
    output:
        country_shapes=RESOURCES + "{interconnect}/country_shapes.geojson",
        onshore_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
    log:
        "logs/build_shapes_{interconnect}.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_shapes.py"

rule build_base_network:
    input:
        buses=DATA + "breakthrough_network/base_grid/bus.csv",
        lines=DATA + "breakthrough_network/base_grid/branch.csv",
        links=DATA + "breakthrough_network/base_grid/dcline.csv",
        bus2sub=DATA + "breakthrough_network/base_grid/bus2sub.csv",
        sub=DATA + "breakthrough_network/base_grid/sub.csv",
        onshore_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
    output:
        bus2sub=DATA + "breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub=DATA + "breakthrough_network/base_grid/{interconnect}/sub.csv",
        network=RESOURCES + "{interconnect}/elec_base_network.nc",
    log:
        "logs/create_network/{interconnect}.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/build_base_network.py"

# TODO: #13 move build_bus_regions higher in the workflow to use base_network
rule build_bus_regions:
    params:
        balancing_areas=config["balancing_areas"],
    input:
        country_shapes= RESOURCES + "{interconnect}/country_shapes.geojson",
        ba_region_shapes=RESOURCES + "{interconnect}/onshore_shapes.geojson",
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        base_network=RESOURCES + "{interconnect}/elec_base_network.nc",
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub="data/breakthrough_network/base_grid/{interconnect}/sub.csv",
    output:
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore.geojson",
        # network=RESOURCES + "{interconnect}/elec_base_network_m.nc",
    log:
        "logs/{interconnect}/build_bus_regions_s.log",
    threads: 1
    resources:
        mem_mb=1000,
    script:
        "scripts/build_bus_regions.py"

rule build_load_data:
    input:
        network=RESOURCES + "{interconnect}/elec_base_network.nc",
        demand_breakthrough_2016=DATA + "breakthrough_network/base_grid/demand.csv",
        ads_2032 = RESOURCES + "WECC_ADS/downloads/2032/Public Data/Hourly Profiles in CSV format",
        ads_2030 = RESOURCES + "WECC_ADS/downloads/2030/WECC 2030 ADS PCM 2020-12-16 (V1.5) Public Data/CSV Shape Files",
        eia = expand(RESOURCES + "eia/{file}", file=DATAFILES_DMD),
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        network=RESOURCES + "{interconnect}/elec_base_network_l.nc",
    log:
        "logs/build_load_data/{interconnect}.log",
    script:
        "scripts/build_load_data.py"

        
if config["enable"].get("build_cutout", False):
    rule build_cutout:
        input:
            onshore_shapes=RESOURCES + "{interconnect}/country_shapes.geojson",
            offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        output:
            cutout= expand("cutouts/" + CDIR + "cutout_western_{cutouts}.nc", **config["atlite"]),
        script:
            "scripts/build_cutout_earth.py"


rule build_ship_raster:
    input:
        ship_density=DATA + "shipdensity_global.zip",
        cutouts=expand(
            "cutouts/" + CDIR + "cutout_western_{cutout}.nc",
            cutout=[
                config["renewable"][carrier]["cutout"]
                for carrier in config["electricity"]["renewable_carriers"]
            ],
        ),
    output:
        RESOURCES + "{interconnect}/shipdensity_raster.tif",
    log:
        LOGS + "{interconnect}/build_ship_raster.log",
    resources:
        mem_mb=5000,
    benchmark:
        BENCHMARKS + "{interconnect}/build_ship_raster"
    script:
        "subworkflows/pypsa-eur/scripts/build_ship_raster.py"


rule build_renewable_profiles:
    params:
        renewable=config["renewable"],
    input:
        base_network=RESOURCES + "{interconnect}/elec_base_network_l.nc",
        corine=ancient("data/pypsa_earth/copernicus/PROBAV_LC100_global_v3.0.1_2019-nrt_Discrete-Classification-map_EPSG-4326.tif"),
        natura=lambda w: (
            DATA + "natura.tiff"
            if config["renewable"][w.technology]["natura"]
            else []
        ),
        gebco=ancient(
            lambda w: (
                DATA + "pypsa_earth/gebco/GEBCO_2021_TID.nc"
                if config["renewable"][w.technology].get("max_depth")
                else []
            )
        ),
        ship_density=lambda w: (
            RESOURCES + "{interconnect}/shipdensity_raster.tif"
            if "ship_threshold" in config["renewable"][w.technology].keys()
            else []
        ),
        country_shapes=RESOURCES + "{interconnect}/country_shapes.geojson", 
        offshore_shapes=RESOURCES + "{interconnect}/offshore_shapes.geojson",
        regions=lambda w: (
            RESOURCES + "{interconnect}/regions_onshore.geojson"
            if w.technology in ("wind", "solar")
            else RESOURCES + "{interconnect}/regions_offshore.geojson"
        ),
        cutout=lambda w: "cutouts/"
        + CDIR + "cutout_{interconnect}_"
        + config["renewable"][w.technology]["cutout"]
        + ".nc",
    output:
        profile=RESOURCES + "{interconnect}/profile_{technology}.nc",
    log:
        LOGS + "{interconnect}/build_renewable_profile_{technology}.log",
    benchmark:
        BENCHMARKS + "{interconnect}/build_renewable_profiles_{technology}"
    threads: ATLITE_NPROCESSES
    resources:
        mem_mb=ATLITE_NPROCESSES * 5000,
    wildcard_constraints:
        technology="(?!hydro).*",  # Any technology other than hydro
    script:
        "subworkflows/pypsa-eur/scripts/build_renewable_profiles.py"

# TODO: Combine add_electricity with build_powerplants 
rule add_electricity:
    params:
        length_factor=config["lines"]["length_factor"],
        scaling_factor=config["load"]["scaling_factor"],
        countries=config["countries"],
        renewable=config["renewable"],
        electricity=config["electricity"],
        conventional=config.get("conventional", {}),
        costs=config["costs"],
    input:
        **{
            f"profile_{tech}": RESOURCES + "{interconnect}" + f"/profile_{tech}.nc"
            for tech in config["electricity"]["renewable_carriers"]
            if tech != "hydro" #ignore hydro becuase we will use other method for attaching hydro
        },
        **{
            f"conventional_{carrier}_{attr}": fn
            for carrier, d in config.get("conventional", {None: {}}).items()
            for attr, fn in d.items()
            if str(fn).startswith("data/")
        },
        base_network=RESOURCES + "{interconnect}/elec_base_network_l.nc",
        tech_costs="repo_data/pypsa_eur_costs.csv",
        regions=RESOURCES + "{interconnect}/regions_onshore.geojson",
        plants="data/breakthrough_network/base_grid/plant.csv",
        hydro="data/breakthrough_network/base_grid/hydro.csv",
        wind="data/breakthrough_network/base_grid/wind.csv",
        solar="data/breakthrough_network/base_grid/solar.csv",
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
    output:
        RESOURCES + "{interconnect}/elec_base_network_l_pp.nc",
    log:
        LOGS + "{interconnect}_add_electricity.log",
    benchmark:
        BENCHMARKS + "{interconnect}_add_electricity"
    threads: 1
    resources:
        mem_mb=5000,
    script:
        "scripts/add_electricity.py"

################# ----------- Rules to Aggregate & Simplify Network ---------- #################
rule simplify_network:
    input:
        bus2sub="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        sub="data/breakthrough_network/base_grid/{interconnect}/sub.csv",
        network= RESOURCES + "{interconnect}/elec_base_network_l_pp.nc",
    output:
        network=RESOURCES + "{interconnect}/elec_s.nc",
    log:
        "logs/simplify_network/{interconnect}/elec_s.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/simplify_network.py"


rule cluster_network:
    input:
        network=RESOURCES + "{interconnect}/elec_s.nc",
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore.geojson",
        busmap="data/breakthrough_network/base_grid/{interconnect}/bus2sub.csv",
        custom_busmap=(
            "data/{interconnect}/custom_busmap_{clusters}.csv"
            if config["enable"].get("custom_busmap", False)
            else []
        ),
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}.nc",
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore_s_{clusters}.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore_s_{clusters}.geojson",
        busmap=RESOURCES + "{interconnect}/busmap_s_{clusters}.csv",
        linemap=RESOURCES + "{interconnect}/linemap_s_{clusters}.csv",
    log:
        "logs/cluster_network/{interconnect}/elec_s_{clusters}.log",
    benchmark:
        "benchmarks/cluster_network/{interconnect}/elec_s_{clusters}"
    threads: 1
    resources:
        mem_mb=6000,
    script:
        "scripts/cluster_network_eur.py"


################# ----------- Rules to Optimize/Solve Network ---------- #################


rule add_extra_components:
    input:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}.nc",
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec.nc",
    log:
        "logs/add_extra_components/{interconnect}/elec_s_{clusters}_ec.log",
    threads: 4
    resources:
        mem=500,
    script:
        "scripts/add_extra_components.py"

rule prepare_network:
    input:
        network=RESOURCES + "{interconnect}/elec_s_{clusters}_ec.nc",
        tech_costs="repo_data/pypsa_eur_costs.csv",
    output:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver="logs/prepare_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.log",
    threads: 4
    resources:
        mem=5000,
    log:
        "logs/prepare_network",
    script:
        "scripts/prepare_network.py"

def memory(w):
    factor = 3.0
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)h$", o, re.IGNORECASE)
        if m is not None:
            factor /= int(m.group(1))
            break
    for o in w.opts.split("-"):
        m = re.match(r"^(\d+)seg$", o, re.IGNORECASE)
        if m is not None:
            factor *= int(m.group(1)) / 8760
            break
    if w.clusters.endswith("m"):
        return int(factor * (18000 + 180 * int(w.clusters[:-1])))
    elif w.clusters == "all":
        return int(factor * (18000 + 180 * 4000))
    else:
        return int(factor * (10000 + 195 * int(w.clusters)))


rule solve_network:
    input:
        RESOURCES + "{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    output:
        "results/{interconnect}/networks/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
    log:
        solver=normpath(
            "logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_solver.log"
        ),
        python="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_python.log",
        memory="logs/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}_memory.log",
    benchmark:
        "benchmarks/solve_network/{interconnect}/elec_s_{clusters}_ec_l{ll}_{opts}"
    threads: 8
    resources:
        mem_mb=memory,
    script:
        "scripts/solve_network.py"


rule plot_figures_single:
    input:
        network="results/{interconnect}/networks/elec_s_{clusters}_ec_l{ll}_{opts}.nc",
        regions_onshore=RESOURCES + "{interconnect}/regions_onshore_s_{clusters}.geojson",
        regions_offshore=RESOURCES + "{interconnect}/regions_offshore_s_{clusters}.geojson",
    output:
        **{
            fig: "results/{interconnect}/figures/elec_s_{clusters}_ec_l{ll}_{opts}_%s.pdf"
            % fig
            for fig in FIGURES_SINGLE
        },
    notebook:
        "notebooks/plot-results.py.ipynb"

rule sync:
    params:
        cluster=config["cluster"],
    shell:
        """
        rsync -uvarh --no-g --ignore-missing-args --files-from=.sync-send . {params.cluster}
        rsync -uvarh --no-g --ignore-missing-args --files-from=.sync-receive {params.cluster} .
        """

rule report:
    message:
        "Compile report."
    input:
        tex="report/report.tex",
        bib="report/references.bib",
    output:
        "report/report.pdf",
    shell:
        """
        pdflatex {input.tex}
        bibtex {input.bib})
        pdflatex {input.tex}
        pdflatex {input.tex}
        """

# Create DAG with- 
# snakemake --dag -F | sed -n "/digraph/,\$p" | dot -Tpng -o repo_data/dag.png
# snakemake --rulegraph all | sed -n "/digraph/,\$p" | dot -Tpng -o repo_data/dag.png
rule dag:
    message:
        "Creating DAG of workflow."
    output:
        dot="repo_data/dag.dot",
        pdf="repo_data/dag.pdf",
        png="repo_data/dag.png",
    shell:
        """
        snakemake --rulegraph all | sed -n "/digraph/,\$p" > {output.dot}
        dot -Tpdf -o {output.pdf} {output.dot}
        dot -Tpng -o {output.png} {output.dot}
        """

rule clean:
    message:
        "Remove all build results but keep downloaded data."
    run:
        import shutil

        shutil.rmtree("resources", ignore_errors=True)
        shutil.rmtree("results", ignore_errors=True)
        print("Data downloaded to data/ has not been cleaned.")
